{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logbook 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monday, January 4\n",
    "I was very bad at keeping up with this during the winter braek even though i still did some work - no sense in trying fruitlessly to cath up now! <br>\n",
    "\n",
    "Brief summary: did correlation, MLS, and variance in winter month comparison of HRDPS and CanRCM4 datasets for 2016,2018,2017,2019 and found that the two datasets correlate SO poorly that it really doesn't make sense to try to downscale using a NN. Worked for a while on making and exporting the preprocessed HRDPS data in another file that would just be run once and never be touched again. Will do the same for PCs. <br>\n",
    "\n",
    "In MOAD meeting today talked about the lack of correlation within the four years that I looked at - was suggested that 2016/2017 winter may have been super weird which is why the model couldn't capture it, but no good reason why the other years look so shit too. SUggested that I look into a larger range of years of data (this will be possible with code that Doug fixed up). <br>\n",
    "\n",
    "Really informative meeting with Doug on what/why he changed to my LoadFiles notebook. Suggested that the removal of the leap day be done after PCA is performed as this dataset will be smaller and easier to work with. Want to load more years of data than we have rn (based on discussion in MOAD meeting) - use the code he has now to make a netcdf file of the HRDPS data with current naming convention (2015 onwards) and a sepratre netcdf for old naming convention (b4 2015) and then concatenate them in the PCA file. <br>\n",
    "Briefly went over how parrallel processing works with dask. Really helpful analogy with clerics counting the number of times a word is used in the bible. The number of clerics (and therefor the number of portions the bible is split into) is the number of processes going on, while the number of words that a cleric can keep track of at once is the number of threads each - this is a simple way of explaining how  computer can 'multitask' using dask. <br>\n",
    "&emsp; NOTE: salish has 16 cores (which looks like 32 when doing what you were (io memory stuff) because there are two threads for each core) so you cannot split your work between more than 32 clerics (less if there is any other work being done on Salish and 16 max if its being used mostly for computing).<br>\n",
    "\n",
    "Some new fun top tricks:<br>\n",
    "&emsp; u -> userID : to see userspecific processes <br>\n",
    "&emsp;&emsp; V : for tree of this users processes <br>\n",
    "&emsp; 1 : to see what all 32 cores are up to<br>\n",
    "&emsp;&emsp; us column = computing memory being used<br>\n",
    "&emsp;&emsp; sy = io memory being used<br>\n",
    "&emsp;&emsp; ni = priority (more posstive is low priority, more negative is high priority, set as 0 automatically)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuesday, January 5\n",
    "Ran and saved HRDPS netCDF files from Sep 2014 to present. Old file types are VERY funky. Some dates have time values from other dates, and the hourly time values within those dates are sometimes not in order. Seems to happen too often to be as simple as me fixing those dates (and still have time to do analysis for the presentation) so just going to do 2014 to present analysis. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wednesday, January 6\n",
    "Lots of progress on downscaling project. Formally moved on from doing downcalling with older HRDPS data. Completed PCA file and exported all the necessary PCA values, time, and lat and lon. <br>\n",
    "Began working on converting larger analysis over to using these exported numpy arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Friday, January 8\n",
    "Productive and informative meeting with Ben this morning on PCA analysis. Learned the proper termanology in our field for some PCA stuff: <br>\n",
    "&emsp;EOF = eigenvectors<br>\n",
    "&emsp;Eigenvalues = fracVar<br>\n",
    "&emsp;Principle Loadings = PCs<br>\n",
    "Also gave me the suggestions to create histograms to show the statistics of what's different between the models.<br>\n",
    "As an aside showed me a file that shows how to make his pretty plot with the outline of the Salish Sea, can be found at this link:https://github.com/SalishSeaCast/analysis-ben/blob/master/notebooks/maps_cartopy.ipynb <br>\n",
    "Also gave me a good book reference for PCA in oceanography: *\"Principle Component Anlysis in Meteorology and Oceanography\" Preisendorfer 1968*<br>\n",
    "\n",
    "Meeting with Susan, she wants me to compare 2014 of climate model to 2014 of SandHeads data. Also suggested to look at switches between high and low and amount of high versus low days in histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saturday, January 9\n",
    "Worked a ton today on completing the 're-vamp' of the SLP file, to amke it compatible with the new PCA and LoadFiles notebooks. Began the same turnover for the wind file. I found a couple errors in the PCA file that lead to the V-wind to be overwritten with the SLP data, so had to fix that before continuing. Wasted a lot of time on trying to combine the first two modes of the wind PCA results upon Ben's suggestion - i should have asked him how exactly he meant to do this (not sure if weighted sum or just regular sum??)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sunday, January 10\n",
    "Thought more about whether or not (and how) to combine the first two modes of the PCA results or jus tot leave them as is and analyse sepertely and i decided to keep them seperate since a) i don't actually know for sure how to combine them correctly (again, ask Ben about this since he definitely eluded that it was quite simple) and b) the eigenvectors of these modes between datasets look very simimilar, to makes resonble sense to comare them directly. <br>\n",
    "Worked the rest of the day on finalising the conversion of the wind file to be compatible with the PCA and LoadFiles notebooks. Added histograms for the whole year and seperate winter of distribution of normalised wind speed and SLP. Made count plot of amount of switches in wind direction and SLP high-lows. <br>\n",
    "Made notebook and plots for comparing winds at one datapoint of the CANRCM4 model to the SandHeads data - this turned out fine but i did it reasonably late at night and i FORGOT TO ADD THE CSV FILES THAT I MADE AND USED FOR THIS NOTEBOOK TO THE .GITIGNORE AND NOW I CAN'T PUSH TO GIT AND NEED TO FIGURE OUT THAT FORGET THING THAT IS APPRENTLY RATHER COMPLICATED UGHHHHHH - maybe i ask Doug for a meeting some time after the presentation to do this together. Just can't upload anyting to git until then... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monday, January 11\n",
    "MOAD meeting in the morning we met the new coop students, i have a prescribed coffee data with Aline. <br>\n",
    "SUggestion to look into where wind is being measured as comparison to SandHeads looks really odd - checked though and measured at 'near surface'.<br>\n",
    "\n",
    "Really interesting stuff from Tereza and Elise stuff today in MOAD meeting: <br>\n",
    "Amount of biomass is small in the JDF (but productivity high) - but out model and others (look at Masson and Pena) show very high biomass. <br>\n",
    "&emsp; - the JDF is a low stratitificaiton region, therefore at the surface chlorophyll is lower but it persists further down than in stratitifed regions <br>\n",
    "&emsp; - so has an overall high biomass signal<br>\n",
    "IMPORTANT: In the model the JDF signal is later in the year than the SoG BUT we usually go out for measurements when the SoG is blooming, therefore may be measuring at times that are biased for low productivity<br>\n",
    "&emsp; - since we don't have observations for the JDF bloom time we can't diffinitively say the model is wrong<br>\n",
    "&emsp; - furthermore, in the model we can't change the impact of light enough (which is said to be why there is lower productivity in the JDF) to get model to match the limited observations we have for both the JDF and SoG<br>\n",
    "\n",
    "This leads us to the question of why does the JDF have a summer bloom instead of a spring bloom?<br>\n",
    "&emsp; - deeper mixing, brings more nutrients to the surface<br>\n",
    "&emsp; - mid-summer incoming solar radiation is maximised, typically deep mixing inhibits the bloom but light can get deeper in mid-summer<br>\n",
    "&emsp; - flow of phytoplankton from the SoG<br>\n",
    "NOTE: super high productivity years may corresnpond to blob years. <br>\n",
    "\n",
    "TA meeting in the afternoon, now rescheduled to Thursdays. <br>\n",
    "\n",
    "Worked on presentation slides. Finished first draft of all slides, need to work on script tomorrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuesday, January 12\n",
    "Worked more on script for presentation and fiddled with slides. Did not finish script before meeting with Susan but did got more suggestions from her on what to include and how to format my slides (so i'm glad i didn't finalize everything first). Other than just basic formating:<br>\n",
    "&emsp;Wants me to add 0 lower limit to varience and major changes graphs so that we aren't 'tricked' into thinking that they are more different then &emsp;they are. <br>\n",
    "&emsp; - show variance formula <br>\n",
    "&emsp;Â - look at summer versus winter variance <br>\n",
    "&emsp; - make sure you are de-meaning the data before analysis it <br>\n",
    "Decided to re-define major changes as diffrence in 0.3 between two days instead of a -ve to +ve change<br>\n",
    "\n",
    "First lectures in methods class - pretty interesting but won't be difficult until afer reading week. Start brainsotrming the type of field measurments you'd like to take/analyse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wednesday, January 13\n",
    "Another meeting with Susan on presentation and added and edited a few other things in the presentaiton. Finished scirpt afterwords and practiced a bit. <br>\n",
    "\n",
    "First class in EOSC 579, going over internal waves first (which you learned a bit about in 471)- just Cuiyi and I so review notes before class because RICK adores asking a ton of questions to us that I apparently remember none of.<br>\n",
    "\n",
    "First class in estruary course - prof is SUPER frednly, spent half the time introducing ourselves. Other half brief intro to Estruaries and density. Didn;t quite get to Knudsen relationship. Finish assignment 1 by next class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thursday, January 14\n",
    "Presented my analysis thus far on the CanRCM4 downscaling infront of the group and members of the DFO - it went adequately. Raised a pretty long discussion afterwords about what to do next. They are actually quite happy with how well the CanRCM4 could capture the variance. How well you need the downscaling to match every storm depends on what you are using this downscaled information for, ex. need to match storms for phytoplankton bloom timing, only needs to match magnitude and varaince for overall circulation study. <br>\n",
    "\n",
    "In EOSC 573 went over what physical oceanographers measure and reasonable ranges of values we can expect to see in the ocean: -2 - 40 deg C for temperature, 0 - 45 g/kg for salinity, and 998 - 1050 kg/m3 for density <br>\n",
    "\n",
    "First of weekly coffee hangs with group members with Aline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Friday, January 15\n",
    "In meeting with Susan we discussed the next steps for me for the downscaling project. Wants to make sure that I am starting to turn my focus onto the JDF. So - will continue the downscaling assessment but will use real wind data from a measurement location in the JDF for comparison with HRDOS data. This way although I am continuing with this project, I will at least be getting more used to how things work in the JDF. <br>\n",
    "PLAN: process a year of station data in JDF (maybe port angeles?) and process the same way as you did the HRDPS data. What is the variance in winter? How does the HRDOS compare? How many EOFs do we need to get 98% of the variance? Do comaprison of wind, wind2 (related to momentum transfer, drag), wind^3 (related to mixing driven by wind). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saturday, January 16\n",
    "Did covid and chemical safety training for EOSC 573. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sunday, January 17\n",
    "Assignment one for CIVL 547 on the equaiton of state. Mostly coded in python. <br>\n",
    "\n",
    "Began reading recomended biological oceanography paper from Tereza on the bloom timing, climactic conditons that bring theseblooms on, and phytoplankton composition of blooms in the NSoG between 2015 and 2018 - *Phytoplankton Composition and Environmental Drivers in the Northern Strait of Georgia (Salish Sea), British Columbia, Canada*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monday, January 18\n",
    "MOAD meeting in the morning, showed those who did not attend my presentation some of the final results of what I presented. Explained that I am going to look more specifically at the JDF now. <br>\n",
    "\n",
    "Reached out to Doug about fixing my github push problem - he will look into it later this week and if he can figure it out will schedule a meeting with me to show me how to fix it. <br>\n",
    "\n",
    "Productive meeting with Tereza on Carbon in the JDF. Sent me an email with some good reasdings fo rme to look at on phytoplankton in the JDF. Exaplined some stuff about her paper and will send me a less 'bare-bones' version that will explain a few things in more depth. Some useful stuff i learned:<br>\n",
    "&emsp; - diatoms and flagelates are the two most important types of phytoplankton in the Salish Sea <br>\n",
    "&emsp; - We measure TA and DIC but in terms of impacts people care about pH, the partial pressure of CO2, and aragonite <br>\n",
    "&emsp; - Can use mocsy (by James Orr) as a good tool in python to do these Ta & DIC conversions to useful things<br>\n",
    "Stressed that we should keep open communication with eachother to make sure there is no overlapp in our research. <br>\n",
    "\n",
    "Formatted and submitted CIVL 547 assignemnt 1 <br>\n",
    "\n",
    "Finished readings on lagrangian vs. eularian vs. tracer perspective by SW Stevens - satrted but still must finish summary writeup. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuesday, January 19\n",
    "Methods course asked Rich about using drifters for my field study. Seemed open to it as long as my proposal is good!<br>\n",
    "\n",
    "Read through the many meeting minutes sent to me from ENVR 400 team - meeting with them went well and gave them some ideas for what their figures could look like. I think they are on the right track and are more realistic now about assigning some sort of economic measurement in order to make their thing usable. Introduced the idea of a \"living document.\" <br>\n",
    "\n",
    "Meeting with Susan, doesn't seem to be worried about overlap with Tereza's work. I am moreso using nitrate and DIC as useful tracers for where the JDF water is coming from and how it is moving whereas Tereza is more explicitely focusing on the transport of carbon. <br>\n",
    "\n",
    "Found good hourly data at two cites on the JDF at Race Rocks and Sheringham Point. DOwnloaded hourly data form these cites back to 2007, still must be processed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wednesday, January 20\n",
    "Conceptually went over assignemnt 1 in CIVL 547. All must re-hand-in assignment 1 and do assignment 2 for next class. Vero reached out to me to work on assignments together! My plot for temperature of amx density with salinity variation is wrong right now - figure out why before working with Vero. <br>\n",
    "\n",
    "Began processing Race Rocks data - figuring out how to do similar processing as i did in the load-files doc but with pandas dataframes instead of netCDF. <br>\n",
    "\n",
    "Re-read EOSC 471 notes on internal waves and re-read all EOSC 512 summary notes to be better prepared for EOSC 579 lecture. Was actually able to answer questions today!! <br>\n",
    "Learned about the behaviour of internal waves between a bottom and surface boundary and the effect of a bumpy bottom. I still don't fully understand what an internal wave mode versus an internal wave ray is - try to figure out before next class. <br>\n",
    "\n",
    "Rich brought up in EOSC 579 lecture that just using drifters alone for methods course would be pretty boring. Suggested combining them with CTD data to see if the flow regimes that the CTD results infer are confirmed or not by the drifters. Need to let him know next week or so if i want to use the drifters as they will need to built (not hard or expensive just takes time) before we go out. <br>\n",
    "Suggested maybe looking at very fast outflow jet along north shore from first narrows. Take CS across across the outflow jet in the top 10 m, drop drifters and pick them up just a few minutes later. <br>\n",
    "Rich will be posting resources for understanding the flow regimes in Burrard inlet soon, so look at that to decide what kind of flow you're interested in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thursday, January 21\n",
    "In methods course learned about biological oceanography field work. For the most part i did not find this that useful BUT learned a bit more detail on the phytoplankton that Tereza highlighted as important in the SoG. <br>\n",
    "&emsp; - Diatoms: opaline skeletal structure, 5000 species that fit into 2 catergories (centric or pennate) based on how they connect together to form long chains<br>\n",
    "&emsp; - Dinoflagellates: 1800 species, mixotrophs, cause of red tides <br>\n",
    "&emsp; - Nanoflagellates: small (< 10 um), mixotrophs, mobile<br>\n",
    "Also learned what is considered ample (2000 uE m-2 s-1) and low (100 uE m-2 s-1) light. Some phytoplankton die right at the surface due to over exposure to light, so the chlorophyll max is typically a little below surface. <br>\n",
    "\n",
    "TA meeting in the afternoon, not much to go over. One of Michaels groups is having problems. Relfection 4 is due this Friday so try to mark over the weekend while you're still not busy. <br>\n",
    "\n",
    "Started working on 'fixed' assignment 1 for CIVL course + first go on assignemnt 2. Still getting the wrong relationship for my temperature of maximum density versus salinity plot and i am not sure why. Hopefully Vero can help tomorrow. <br>\n",
    "Think that i got question 5 and 6 right for assignment 1 (I think people in class are wayyy over complicating it) and assignemnt 2 i think i'm done as well, although i made an assumption with regards to density that I should run by Vero tomorrow as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Friday, January 22\n",
    "Worked on RaceRocks and Sheringham analysis in the morning. Racerocks has fewer gaps in the data so I am thinking of focusing on that. Also it is on an island so has higher winds than the coastal values of Sheringham which I think are more useful for what I am attempting. <br>\n",
    "\n",
    "Met with Vero at 11 to go over our work so far. She is not very comfortable with python so could not help with problem in my code but we talked over our assumptions and process together to make sure that we are on the right track. She gave me the idea to use the density relationships we derove in assignment 1 instead of making assumptions about them for assignemnt 2 (still must assume a temperature fo the water but I think this will be more accurate). We sent eachother our script/sheet and will try to help eachother out. Excited to have a friend in this class already to work with!<br>\n",
    "\n",
    "Meeting with Susan she explained to me why the channel between Vancouver Island and Lasqueti Island was rejected as the path where trasport occurs in Sam Steven's paper even though that's what the Lagrangian modelling said was happening. The model does not have the high resolution bathymetry information in it (since there is no way right now to put this much detail in) so based on the low-resolution bathymetry it has, it thinks that this channel is a good choice (since it it wide) BUT in reality this channel is super shallow so it is unlikely that it is the main pathway. Hypothesied by Rich (but not yet confirmed) that a thinner but much deeper channel to the east is the main pathway. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sunday, January 24\n",
    "Worked on CIVL assignment 1 again, and after a few days of not looking at it noticed that I had forgot to put -ve's in front of two of the b-coefficents and this is why my relationship was off! Everyhting working now so i was able to make all of the plots look pretty and put a word doc togther to sumit both assignment 1 and 2. <br>\n",
    "\n",
    "Trimmed the HRDPS data to only look at the point closest to RaceRocks. I realised after this though that the HRDPS data I am working with right now is the daily average. This is a step I took to make it match the CanRCM4 data but doesn't make sense to use for comparing to the RaceRocks data which is hourly. Must go back to old LoadFiles and PCA notebooks and redo them for hourly data processing. <br>\n",
    "\n",
    "Got over half-way through grading ENVR 400 reflection 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monday, January 25\n",
    "Finished marking ENVR 400 reflection 4 this morning. <br>\n",
    "\n",
    "In MOAD meeting Susan let me know about a meeting she had with the DFO on the donscaling project. A previous student from the DFO was part of the meeting (Alex Cannon) who works in statistical machine learning in this field and explained a way to match the models using their statistics instead of their storms. Seems really cool, I look forward to hearing more about it. <br>\n",
    "Through this conversation i also FINALLY figured out what Susan wants me to do rn. Basically reconstruct the HRDPS data using your PCA results (the hourly ones) and see how well the reconstructed point closest to Race Rocks matches Race Rocks. Interested to see if there is a good match and if there is, how many modes it takes to get this good match. <br>\n",
    "\n",
    "Worked on redoing the LoadFiles and PCA analysis on hourly HRDPS data. Was able to export a netCDF successfully using a new version of LoadFiles but still need to complete the PCA. I believe that my re-worked PCA file will run without a problem but need to wait until this evenning as processing years of hourly data takes up a lot more space than processing the daily data and I think it would be best to run it on the server when no one else is on salish. Note that I am now also doing the conversion to wind speed and direction (instead of U and V) in this file. Remeber to export the seasonal trand values that you subtract so that you can get back to the full values later. <br>\n",
    "\n",
    "In PO seminar Rick gave an interesting presentation on his drifters and why they may ground so much in the SoG but not in the St.Lawrence. Drifters staying much closer to shore than expected (seeing log-layer relationship in theit distnace from shore relationship). Found rn that Stokes drift and tides are not the reason. <br>\n",
    "Ask him to talk in class about the internal waves in the South China Sea around the Dongsha Atoll. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuesday, January 26\n",
    "Run overnight on hourly version of PCA did not work. Tried again this morning with commenting out all pressure parts and running that (then will comment out all direction parts and do that too) and i think that will work. Doug has to run a big thing though so need to wait a couple days. <br>\n",
    "Susan suggested that i just start with making racerocks daily and comapring that way. Didn't get as far in that new process as I would like to have but seems very doable. Having some problems with my HRDPS data not being the right length (off by about 300 days) and I'm not sure why this is happening since I am using almost the same process as I did in the big wind analysis file. Look into this tomorrow. <br>\n",
    "\n",
    "Methods lecture on biological oceanography. Expanded into zooplankton. Nothing too exciting to note. <br>\n",
    "\n",
    "Started on pre-proposal file for methods project. Want to analysis the jet along the north shore of English Bay using both drifters and CTD, but I am not actually sure why this research is useful in any way so far. Potentially can use it to determine the accuracy of SalishSeaCast FVCOM results in that area but I am not sure if that is useful. Thus idea came from cool SalishSeaCast graphics of English Bay at https://salishsea.eos.ubc.ca/fvcom/results/nowcast-r12/publish/22jan21 <br>\n",
    "Ask Rich to speak about this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wednesday, January 27\n",
    "In estuary course learned that he wanted us to use equations from his paper not from this figure from the same picture, so must redo assignment sections with more precise determination. Met with vero for a bit after class to go over this.<br>\n",
    "Went over momentum equations in class and for estuaries (instad of more general ones covered in GFD) look a lot more different than I thought they would. Go over this again b4 next class (must solve for assignment 3 anyways). <br>\n",
    "\n",
    "Met with Elise for coffee hang, t'was nice. <br>\n",
    "\n",
    "Redid code for assignment 2 in estuaries class using Guha & Lawrence JPO 2013. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thursday, January 28\n",
    "Re-wrote and formatted assignment 2 for CIVL. <br>\n",
    "\n",
    "Made reconstruction for downscaling project to compare to Racerocks. Ready to show Susan some stuff and get her comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Friday, January 29\n",
    "Re-read Thompson 1981 section on Burrard inlet in an effort to get a better idea of what i want to address in my methods project. I spoke about this later with Susan and she suggested that my 'why' could be looking into why oil spills in the inlet effec the north shore more than the south. Also very difficult for sailors to navigate that area. Additionally still cool and interesting to compare it to VHFR FVCOM results. Will work on the-preproposal this weekend toget it to Rich by monday. <br>\n",
    "\n",
    "Call with Susan, in addition to talking about my methods project, was productive. Learned that my reconstruction of the wind direction my not be wrong.. oceanographer and atmospheric scientists measure winds oppositely so its just a perspective thing - flip the recerocks values. For interest, plot the raw hrdps data versus the racerocks data to see how well that matches up before comparing reconstruction. <br>\n",
    "Susan thinking that since wind direction modes dont match up as well (as expected) since direction is the hardest to get accurately between models that she may combine the U and V wind speeds into one dataset with just twice the spatial area and do my analysis on that. She showed me some of the stats taht she's been doing on the data, and once you normalise both datasets by their variances they show really similar behaviour. <br>\n",
    "Susan to send my the ocean parcels (a simpler version of arianne) code to start playing around with. <br>\n",
    "\n",
    "In EOSC 579 i took up about half the lecture asking Rich the difference between an internal wave mode and an internal wave ray. Extremely helpful but i didn't write notes since i wanted to take as much in as possible and stay alert. Re-watch video and jot down some notes for reference in the future. <br>\n",
    "\n",
    "Worked on CIVL asssignment 2 and was able to complete what i think he wants for question 1 (derivation of the along strait velocity) much more easily than I thought I would be able to. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sunday, January 31\n",
    "Read the relevant section of MacCready & Geyer (2010) for assignment 3 question 2 in CIVL. Met with Vero to talk about the assignment. We arrived at the same answer slightly differently for question 1 and both agree that for question 2 we just need to write MacCready & Geyer (2010) derivation of the salt flux equation in our own words. <br>\n",
    "\n",
    "Wrote up pre-proposal for moethods course to send to Rich tomorrow morning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monday, February 1\n",
    "Implemented suggested extra info for Rich into my pre-proposal (added tide tables to show more precisely when the edd tide occurs). <br>\n",
    "\n",
    "Flipped wind data and added plot of original HRDPS data into data analysis code. Wind data in HRDPS doesn't like to blow west as much as the Race Rocks data and doesn't reach as high of magnitudes. Ben showed me his wind rose for race rocks during the MOAD meeting and it agrees with this analysis. Look at his code more and see what you can learn from it (even if it is just making your code more neet). <br>\n",
    "\n",
    "Wrote up question 2 for CIVL assignemnt - I attempted to write the steps and assumption in my own words but summarising a summary just becomes very paraphrasy. Look into the other papers referenced in this review - in particular MacCready 1999 and 2004. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
