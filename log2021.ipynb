{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logbook 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monday, January 4\n",
    "I was very bad at keeping up with this during the winter braek even though i still did some work - no sense in trying fruitlessly to cath up now! <br>\n",
    "\n",
    "Brief summary: did correlation, MLS, and variance in winter month comparison of HRDPS and CanRCM4 datasets for 2016,2018,2017,2019 and found that the two datasets correlate SO poorly that it really doesn't make sense to try to downscale using a NN. Worked for a while on making and exporting the preprocessed HRDPS data in another file that would just be run once and never be touched again. Will do the same for PCs. <br>\n",
    "\n",
    "In MOAD meeting today talked about the lack of correlation within the four years that I looked at - was suggested that 2016/2017 winter may have been super weird which is why the model couldn't capture it, but no good reason why the other years look so shit too. SUggested that I look into a larger range of years of data (this will be possible with code that Doug fixed up). <br>\n",
    "\n",
    "Really informative meeting with Doug on what/why he changed to my LoadFiles notebook. Suggested that the removal of the leap day be done after PCA is performed as this dataset will be smaller and easier to work with. Want to load more years of data than we have rn (based on discussion in MOAD meeting) - use the code he has now to make a netcdf file of the HRDPS data with current naming convention (2015 onwards) and a sepratre netcdf for old naming convention (b4 2015) and then concatenate them in the PCA file. <br>\n",
    "Briefly went over how parrallel processing works with dask. Really helpful analogy with clerics counting the number of times a word is used in the bible. The number of clerics (and therefor the number of portions the bible is split into) is the number of processes going on, while the number of words that a cleric can keep track of at once is the number of threads each - this is a simple way of explaining how  computer can 'multitask' using dask. <br>\n",
    "&emsp; NOTE: salish has 16 cores (which looks like 32 when doing what you were (io memory stuff) because there are two threads for each core) so you cannot split your work between more than 32 clerics (less if there is any other work being done on Salish and 16 max if its being used mostly for computing).<br>\n",
    "\n",
    "Some new fun top tricks:<br>\n",
    "&emsp; u -> userID : to see userspecific processes <br>\n",
    "&emsp;&emsp; V : for tree of this users processes <br>\n",
    "&emsp; 1 : to see what all 32 cores are up to<br>\n",
    "&emsp;&emsp; us column = computing memory being used<br>\n",
    "&emsp;&emsp; sy = io memory being used<br>\n",
    "&emsp;&emsp; ni = priority (more posstive is low priority, more negative is high priority, set as 0 automatically)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuesday, January 5\n",
    "Ran and saved HRDPS netCDF files from Sep 2014 to present. Old file types are VERY funky. Some dates have time values from other dates, and the hourly time values within those dates are sometimes not in order. Seems to happen too often to be as simple as me fixing those dates (and still have time to do analysis for the presentation) so just going to do 2014 to present analysis. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wednesday, January 6\n",
    "Lots of progress on downscaling project. Formally moved on from doing downcalling with older HRDPS data. Completed PCA file and exported all the necessary PCA values, time, and lat and lon. <br>\n",
    "Began working on converting larger analysis over to using these exported numpy arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Friday, January 8\n",
    "Productive and informative meeting with Ben this morning on PCA analysis. Learned the proper termanology in our field for some PCA stuff: <br>\n",
    "&emsp;EOF = eigenvectors<br>\n",
    "&emsp;Eigenvalues = fracVar<br>\n",
    "&emsp;Principle Loadings = PCs<br>\n",
    "Also gave me the suggestions to create histograms to show the statistics of what's different between the models.<br>\n",
    "As an aside showed me a file that shows how to make his pretty plot with the outline of the Salish Sea, can be found at this link:https://github.com/SalishSeaCast/analysis-ben/blob/master/notebooks/maps_cartopy.ipynb <br>\n",
    "Also gave me a good book reference for PCA in oceanography: *\"Principle Component Anlysis in Meteorology and Oceanography\" Preisendorfer 1968*<br>\n",
    "\n",
    "Meeting with Susan, she wants me to compare 2014 of climate model to 2014 of SandHeads data. Also suggested to look at switches between high and low and amount of high versus low days in histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saturday, January 9\n",
    "Worked a ton today on completing the 're-vamp' of the SLP file, to amke it compatible with the new PCA and LoadFiles notebooks. Began the same turnover for the wind file. I found a couple errors in the PCA file that lead to the V-wind to be overwritten with the SLP data, so had to fix that before continuing. Wasted a lot of time on trying to combine the first two modes of the wind PCA results upon Ben's suggestion - i should have asked him how exactly he meant to do this (not sure if weighted sum or just regular sum??)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sunday, January 10\n",
    "Thought more about whether or not (and how) to combine the first two modes of the PCA results or jus tot leave them as is and analyse sepertely and i decided to keep them seperate since a) i don't actually know for sure how to combine them correctly (again, ask Ben about this since he definitely eluded that it was quite simple) and b) the eigenvectors of these modes between datasets look very simimilar, to makes resonble sense to comare them directly. <br>\n",
    "Worked the rest of the day on finalising the conversion of the wind file to be compatible with the PCA and LoadFiles notebooks. Added histograms for the whole year and seperate winter of distribution of normalised wind speed and SLP. Made count plot of amount of switches in wind direction and SLP high-lows. <br>\n",
    "Made notebook and plots for comparing winds at one datapoint of the CANRCM4 model to the SandHeads data - this turned out fine but i did it reasonably late at night and i FORGOT TO ADD THE CSV FILES THAT I MADE AND USED FOR THIS NOTEBOOK TO THE .GITIGNORE AND NOW I CAN'T PUSH TO GIT AND NEED TO FIGURE OUT THAT FORGET THING THAT IS APPRENTLY RATHER COMPLICATED UGHHHHHH - maybe i ask Doug for a meeting some time after the presentation to do this together. Just can't upload anyting to git until then... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
